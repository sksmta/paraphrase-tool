{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Paraphrase Tool \n",
    "\n",
    "This is a simple tool to paraphrase text using the [__*ramsrigouthamg/t5-large-paraphraser-diverse-high-quality*__](https://huggingface.co/ramsrigouthamg/t5-large-paraphraser-diverse-high-quality) pre-trained model, provided via [huggingface](https://huggingface.co/). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing the necessary packages \n",
    "\n",
    "We require transformers (to communicate with huggingface) and sentencepiece, which the t5 model uses.\n",
    "\n",
    "*Protobuf* is not a required module to install, if you're using google colab. I ran this on github codespaces, therefore protobuf was not pre-present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install sentencepiece \n",
    "%pip install transformers\n",
    "%pip install protobuf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the two necessary libraries\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Paraphrase Class \n",
    "\n",
    "In all simplicity, we're using a pre-trained model in order to rewrite phrases. \n",
    "\n",
    "`self.device` is set to \"cuda\" for gpu usage, if torch is unable to find the gpu, it uses cpu instead. \n",
    "\n",
    "**The rewrite fuction** outputs a `list` of paraphrased sentences to choose the most appropriate (grammatically & meaning-wise). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Paraphrase: \n",
    "    def __init__(self): \n",
    "       self.model = AutoModelForSeq2SeqLM.from_pretrained(\"ramsrigouthamg/t5-large-paraphraser-diverse-high-quality\") # This model is trained on a large dataset of paraphrases and is able to generate high quality paraphrases.\n",
    "       self.tokenizer = AutoTokenizer.from_pretrained(\"ramsrigouthamg/t5-large-paraphraser-diverse-high-quality\")\n",
    "       self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # use gpu if available else use cpu\n",
    "       self.model = self.model.to(self.device) # load model into gpu if available else load into cpu\n",
    "    \n",
    "    # Simple rewrite function, takes in a string and returns a list of paraphrased sentences\n",
    "    def rewrite(self, text) -> list :\n",
    "       encoding = self.tokenizer.encode_plus(text,max_length =128, padding=True, return_tensors=\"pt\") # encode input text into tokenized ids and attention mask tensors \n",
    "       input_ids,attention_mask  = encoding[\"input_ids\"].to(self.device), encoding[\"attention_mask\"].to(self.device) # load tensors into gpu if available else load into cpu \n",
    "       self.model.eval() # set model to evaluation mode\n",
    "       # generate paraphrases using beam search with beam size 5, beam groups 5, and diversity penalty 0.70\n",
    "       diverse_beam_outputs = self.model.generate(\n",
    "       input_ids=input_ids,attention_mask=attention_mask, \n",
    "       max_length=128, # maximum length of generated paraphrase (can be changed)\n",
    "       early_stopping=True, # stop generation when all beam hypotheses reach end of sentence token (EOS)\n",
    "       num_beams=5, # number of beams to use for beam search (Beam search is a heuristic search algorithm that explores a graph by expanding the most promising node in a limited set. It is a greedy algorithm that expands the search space by one node in all possible directions.)\n",
    "       num_beam_groups = 5,\n",
    "       num_return_sequences=5,\n",
    "       diversity_penalty = 0.70) # higher penalty means more diverse paraphrases\n",
    "       phc = [] \n",
    "       for beam_output in diverse_beam_outputs: # iterate through each paraphrased sentence\n",
    "             sent = self.tokenizer.decode(beam_output, skip_special_tokens=True,clean_up_tokenization_spaces=True) # decode tokenized ids into paraphrased sentence. Skips special tokens and cleans up tokenization spaces\n",
    "             if sent.lower() != text.lower() and sent not in phc:\n",
    "                phc.append(sent)\n",
    "       return phc\n",
    "       \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets run the model! \n",
    "\n",
    "- Your output might be something like: \n",
    "\n",
    "```json\n",
    "[\"paraphrasedoutput: I don't even know if I'll make it to the party.\", ....]\n",
    "```\n",
    "- As you might infer from this, we need to remove *paraphrasedoutput* from the array of strings.\n",
    "\n",
    "- So we create a function called `extract_sentences`, that basically extracts the string \"paraphrasedoutput\" from each item (str) in the array.\n",
    "\n",
    "- Next we need to find out the most appropriate paraphrase out of the array of sentences. \n",
    "  And to do just that, we use a library called `language-tool-python`\n",
    "\n",
    "- `language-tool-python` basically checks the grammatically outline of the sentences and chooses the most apt out of the lot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# Run model\n",
    "\n",
    "para = Paraphrase() # Load model\n",
    "stxs = para.rewrite(text) # Rewrite text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install language-tool-python # install language tool for grammar check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the sentences from the json string output \n",
    "\n",
    "def extract_sentences(sentences) -> list :\n",
    "    sens = [] \n",
    "    for sentence in sentences: \n",
    "        cleaned_sentence = sentence.replace(\"paraphrasedoutput:\", \"\").strip() # remove the prefix \n",
    "        sens.append(cleaned_sentence)\n",
    "    return sens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from language_tool_python import LanguageTool\n",
    "\n",
    "def appropriation(sentences):\n",
    "    tool = LanguageTool('en-US')  # Grammar checker\n",
    "\n",
    "    best_sentence = \"\"\n",
    "    best_grammar_score = float('-inf')\n",
    "\n",
    "    for sentence in sentences:\n",
    "     matches = tool.check(sentence)\n",
    "     grammar_score = len(matches)\n",
    "    \n",
    "    if grammar_score > best_grammar_score:\n",
    "        best_grammar_score = grammar_score\n",
    "        best_sentence = sentence\n",
    "    \n",
    "    return best_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "btx = extract_sentences(stxs)\n",
    "print(appropriation(btx))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
